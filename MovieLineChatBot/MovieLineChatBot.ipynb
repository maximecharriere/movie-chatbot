{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Lines Chat Bot\n",
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import string\n",
    "import collections\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pprint import pprint\n",
    "import math\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import lines \n",
    "url_movie_line = \"https://raw.githubusercontent.com/maximecharriere/movie-chatbot/master/MovieLineChatBot/data/outLines.txt\"\n",
    "movie_lines = pd.read_csv(url_movie_line, sep='\\+{3}\\$\\+{3}', engine='python', names=(\"First line\",\"Reply\", \"Film\", \"Character\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to process text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    cleanedLines = \"\".join([c for c in text if c not in string.punctuation])\n",
    "    return cleanedLines\n",
    "\n",
    "# remove stop words\n",
    "def remove_stopwords(text):\n",
    "    words = [w for w in text if w not in stopwords.words('english')]\n",
    "    return words\n",
    "\n",
    "# lemmatizer\n",
    "def word_lemmatizer(text):\n",
    "    cleanedText = [lemmatizer.lemmatize(i) for i in text]\n",
    "    return cleanedText\n",
    "\n",
    "# Join words\n",
    "def join_text(text):\n",
    "    stem_text = \" \".join(text)\n",
    "    return stem_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the text and extracting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataLen = len(movie_lines[\"First line\"])\n",
    "linesVectors = np.zeros((dataLen, 96))\n",
    "\n",
    "for i in range(dataLen):\n",
    "    if(not pd.isnull(movie_lines[\"First line\"][i])):\n",
    "        parsedText = remove_punctuation(movie_lines[\"First line\"][i])\n",
    "        parsedText = tokenizer.tokenize(parsedText.lower())\n",
    "        parsedText = remove_stopwords(parsedText)\n",
    "        parsedText = join_text(parsedText)\n",
    "        line = nlp(parsedText)\n",
    "        if(len(line.vector) != 0):\n",
    "            linesVectors[i][:] = line.vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised clustering of the text vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='full', copy_x=True, init='random', max_iter=3000,\n",
       "       n_clusters=600, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km_model = KMeans(n_clusters=600, init='random', max_iter=3000, algorithm='full')\n",
    "km_model.fit(linesVectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process user text, predict and reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User :  Spare me you excuses, why are you such a disappointment?\n",
      "Bot  :  I'm a photographer, remember?  -  PETER  from  spider-man \n"
     ]
    }
   ],
   "source": [
    "message  = \"Spare me you excuses, why are you such a disappointment?\"\n",
    "parsedText = remove_punctuation(message)\n",
    "parsedText = tokenizer.tokenize(parsedText.lower())\n",
    "parsedText = remove_stopwords(parsedText)\n",
    "parsedText = join_text(parsedText)\n",
    "line = nlp(parsedText)\n",
    "\n",
    "clustering = collections.defaultdict(list)\n",
    "for idx, label in enumerate(km_model.labels_):\n",
    "        clustering[label].append(idx)\n",
    "        \n",
    "clusterIndex = km_model.predict([line.vector])[0]\n",
    "maxIndex = len(clustering[clusterIndex])\n",
    "randIndex = np.random.randint(0, maxIndex)\n",
    "lineIndex = clustering[clusterIndex][randIndex]\n",
    "\n",
    "print(\"User : \", message)\n",
    "print(\"Bot  :\", movie_lines[\"Reply\"][lineIndex], \" -\",  movie_lines[\"Character\"][lineIndex], \" from\", movie_lines[\"Film\"][lineIndex])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
